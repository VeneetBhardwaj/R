---
title: "Case Study: JPL Small Body Classification in R"
author: "Veneet Bhardwaj" 
email: "veneet.bhardwaj@gmail.com"
date: "05/01/2021"
fontsize: 9
output:
  pdf_document: 
    toc: yes
    fig_caption: yes
    number_sections: yes
    keep_tex: yes
    toc_depth: 4
    fig_crop: no
  html_document: default
---
\newpage

# Introduction


A $NEO$, Near Earth Object, is a small Solar System body (an asteroid or comet), which comes in proximity with Earth during its orbit.

A $PHO$, Potentially Hazardous Object, is a NEO with an orbit that intersects that of Earth and hence there is a probability of a collision.

When a $NEO$ is detected, like all other small Solar System bodies, its positions and brightness are submitted to the International Astronomical Union (IAU) Minor Planet Center (MPC) for cataloging. The MPC maintains separate lists of confirmed NEOs and potential NEOs.

NEOs are also cataloged by two separate units of the Jet Propulsion Laboratory (JPL) of the National Aeronautics and Space Administration (NASA): the Center for Near Earth Object Studies (CNEOS) and the Solar System Dynamics Group.


**This case study is on making a classification model using R, with the JPL dataset.**


## Objective

$\rightarrow$ The **Objective** of this case study to build a machine learning model to predict the classification of  the small solar bodies into $PHO$, $NEO$ and $SO$ (other small solar bodies).

Target measures :

+   Recall for each category above 0.99

+   F1 Score for each category above 0.99


The dataset used is the data available at : 

**https://ssd.jpl.nasa.gov/sbdb_query.cgi**

\hfill

_ Downloaded file : Nasa_JPL_SmallBody.csv ~555Mb in size _

_ Compressed split files have been uploaded in the folder https://raw.githubusercontent.com/VeneetBhardwaj/R/ _




\newpage

## A Brief History 
_source wikipedia_

Comets were the first near-Earth objects to be observed by us.

Their extraterrestrial nature was recognized and confirmed only after Tycho Brahe tried to measure the distance of a comet through its parallax in 1577 and the lower limit he obtained was well above the Earth diameter.

+   In 1705, Edmond Halley published his orbit calculations of a NEO, the Halley Comet. The 1758â€“1759 return of Halley Comet was the first comet appearance that had been predicted. 
+   It has been said that Lexell comet of 1770 was the first discovered Near-Earth object.
+   The first near-Earth asteroid to be discovered was 433 Eros in 1898.
+   As of January 2019, five near-Earth comets and five near-Earth asteroids have been visited by spacecraft. A small sample of one NEO was returned to Earth in 2010, and similar missions are in progress. 

The first astronomical program dedicated to the discovery of near-Earth asteroids was the Palomar Planet-Crossing Asteroid Survey, started in 1973 by astronomers Eugene Shoemaker and Eleanor Helin.

Several surveys have undertaken the NEO detection. 

+   Lincoln Near-Earth Asteroid Research (LINEAR)
+   Spacewatch, Near-Earth Asteroid Tracking (NEAT)
+   Lowell Observatory Near-Earth-Object Search (LONEOS)
+   Catalina Sky Survey (CSS)
+   Campo Imperatore Near-Earth Object Survey (CINEOS)
+   Japanese Spaceguard Association
+   Asiago-DLR Asteroid Survey (ADAS)
+   Near-Earth Object WISE (NEOWISE). 

In 2005, the original USA Spaceguard mandate to detect 90% of near-earth asteroids over 1 km (0.62 mi) diameter, was extended by the George E. Brown, Jr. Near-Earth Object Survey Act, which calls for NASA to detect 90% of NEOs with diameters of 140 m (460 ft) or greater, by 2020.

\includegraphics{one.jpg}

As of January, 2020, it is estimated that less than half of these have been found. NASA maintains an automated system to evaluate the threat from known NEOs over the next 100 years, which generates the continuously updated Sentry Risk Table. 

\newpage

# Analysis
```{r message=FALSE, warning=FALSE, include=FALSE}
rm(list = ls(all.names=TRUE))
gc()
```

```{r message=FALSE, warning=FALSE, include=TRUE}
#load the required libraries
library(knitr)
library(markdown)
options(digits = 5)
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(Hmisc)) install.packages("Hmisc", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
if(!require(naivebayes)) install.packages("naivebayes", repos = "http://cran.us.r-project.org")
if(!require(ranger)) install.packages("ranger", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(wsrf)) install.packages("wsrf", repos = "http://cran.us.r-project.org")
```


## Data setup

The JPL small body data set is downloaded from  **https://ssd.jpl.nasa.gov/sbdb_query.cgi**


The downloaded file is Nasa_JPL_SmallBody.csv (~555Mb)

```{r Download, message=FALSE, warning=FALSE, include=FALSE}
# Please NOTE:
# THIS CODE CHUNK WILL DOWNLOAD THE FILES REQUIRED BY THIS RMD, if not present.
# THE FILES WILL BE DOWNLOADED FROM A GITHUB REPOSITORY.
# DOWNLOADED FILES ARE ~ 223MB

#####TO DOWNLOAD MANUALLY PLEASE STOP EXECUTION OF THIS CHUNK #####
#####AND RUN THE "manualDownload" CHUNK, THE NEXT ONE.#####

# NO FILES WILL BE DELETED. ONCE THIS CHUNK HAS COMPLETED 
# THE JPL_1.zip,JPL_2.zip,JPL_3.zip CAN BE REMOVED. 

# ALL OTHER FILES ARE REQUIRED FOR THE REMAINING CODE EXECUTION.
# approximately ~560MB OF DISK SPACE IS REQUIRED.


if(file.exists("Nasa_JPL_SmallBody.csv") != TRUE){ 
#Download the required files

dl <- tempfile()
download.file("https://raw.githubusercontent.com/VeneetBhardwaj/R/main/JPLSBA/JPL_1.zip",dl)
#sha256sum should be F156FD5A7AF7202145D7B21F1B40BA62626F8192D46CAC290A0ABC9CD2A8DBEE
dat1<- read_csv(dl)
rm(dl)

dl <- tempfile()
download.file("https://raw.githubusercontent.com/VeneetBhardwaj/R/main/JPLSBA/JPL_2.zip",dl)
#sha256sum should be 2603A25A4C44973FB0C7C0A607980C8D992538AF55A247DAE815483C708CBDB8
dat2<- read_csv(dl)
rm(dl)

dl <- tempfile()
download.file("https://raw.githubusercontent.com/VeneetBhardwaj/R/main/JPLSBA/JPL_3.zip",dl)
#sha256sum should be ECDE408DD9D89396F22F2D9AAB2015F3FC6EA688832B3C8CC86D884EBA1924E8
dat3<- read_csv(dl)
rm(dl)

dat <- bind_rows(dat1,dat2,dat3)
write_csv(dat,'Nasa_JPL_SmallBody.csv')
rm(dat1,dat2,dat3,dat)
}

if(file.exists("JPLdesc.csv") != TRUE){ 
download.file("https://raw.githubusercontent.com/VeneetBhardwaj/R/main/JPLSBA/JPLdesc.csv",
              "JPLdesc.csv")}

if(file.exists("one.jpg") != TRUE){ 
download.file("https://raw.githubusercontent.com/VeneetBhardwaj/R/main/JPLSBA/one.jpg",
              "one.jpg")}

if(file.exists("two.jpg") != TRUE){ 
download.file("https://raw.githubusercontent.com/VeneetBhardwaj/R/main/JPLSBA/two.jpg",
              "two.jpg")}

```


```{r manualDownload, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Please NOTE:
# THIS CODE CHUNK COMBINES THE FILES REQUIRED FOR THIS RMD

GIT_LINK = "https://raw.githubusercontent.com/VeneetBhardwaj/R/main/JPLSBA/"

#PLEASE DOWNLOAD ALL THE FILES FROM THE FOLDER ABOVE TO THE CURRENT FOLDER.
# DOWNLOADED FILES ARE ~ 223MB

# ALL OTHER FILES ARE REQUIRED FOR THE REMAINING CODE EXECUTION.
# approximately ~560MB + 233 MB (downloaded) OF DISK SPACE IS REQUIRED.
# NO FILES WILL BE DELETED. ONCE THIS CHUNK HAS COMPLETED 
# THE JPL_1.zip,JPL_2.zip,JPL_3.zip CAN BE REMOVED. 

if(file.exists("JPL_1.zip") != TRUE) cat("\n Please Download JPL_1.zip \n")
if(file.exists("JPL_2.zip") != TRUE) cat("\n Please Download JPL_2.zip \n")
if(file.exists("JPL_3.zip") != TRUE) cat("\n Please Download JPL_3.zip \n")
if(file.exists("JPLdesc.csv") != TRUE) cat("\n Please Download JPLdesc.csv \n") 
if(file.exists("one.jpg") != TRUE) cat("\n Please Download one.jpg \n") 
if(file.exists("two.jpg") != TRUE) cat("\n Please Download two.jpg \n") 

if(file.exists("Nasa_JPL_SmallBody.csv") != TRUE){ 
    #unzip the required files
    if(file.exists("JPL_1.zip") == TRUE & 
       file.exists("JPL_2.zip") == TRUE &
       file.exists("JPL_3.zip") == TRUE ){ 
    
    #JPL_1.zip
    #sha256sum should be F156FD5A7AF7202145D7B21F1B40BA62626F8192D46CAC290A0ABC9CD2A8DBEE
    dat1<- read_csv("JPL_1.zip")
    
    #JPL_2.zip
    #sha256sum should be 2603A25A4C44973FB0C7C0A607980C8D992538AF55A247DAE815483C708CBDB8
    dat2<- read_csv("JPL_2.zip")
    
    #JPL_3.zip
    #sha256sum should be ECDE408DD9D89396F22F2D9AAB2015F3FC6EA688832B3C8CC86D884EBA1924E8
    dat3<- read_csv("JPL_3.zip")
    
    dat <- bind_rows(dat1,dat2,dat3)
    write_csv(dat,'Nasa_JPL_SmallBody.csv')
    rm(dat1,dat2,dat3,dat,GIT_LINK)
    }
  }

```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Please NOTE:
# Some parts of the following code chunks are configured to run on a 6+ core computer.
# Please reconfigure num.threads and the n.cores parameters of the ranger and gbm models accordingly.
```

```{r dataload, message=FALSE, warning=FALSE, include=TRUE}
data <- data.table::fread("Nasa_JPL_SmallBody.csv", na.strings = c(NA,"NA",""))
dim(data)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
gc()
```

There are 1081059 observations (rows) across 59 columns in the dataset.

```{r message=FALSE, warning=FALSE, include=TRUE}
names(data) 
```


Brief description on the columns of the data set are shown below. These have been _collated from the **https://ssd.jpl.nasa.gov** website_. 

```{r message=FALSE, warning=FALSE, include=TRUE}
#load the descriptions csv.
data$id <- str_trim(data$id)
desc_d <- as.data.frame(data.table::fread("JPLdesc.csv"))
desc_d[,2:4] %>% knitr::kable()
```

Observations on the variables (columns) property in the dataset:

+   The intent of this case study is to classify the small body objects into the appropriate flags of neo Near-Earth Object (Neo) Flag	and pha	Potentially Hazardous Asteroid (Pha) Flag.

+   Some columns are descriptive of the object like _Object full name_ and _name of the person who computed the orbit_.

+   Some are regarding objects observation properties, like the _number of observations used for orbit fit_, _date of first and last observation_ etc..

+   There are also the 1-Sigma Uncertainty of the values in the data set.

+   There are variables which are  orbital observations and the observations of the object properties. These are the variables which will be used in making the classification model.



\newpage
## Outcome Variable

**The Outcome Variable** (to be predicted by the model) needs to be setup from a combination of 2 columns - $pha$ and $neo$. Both these have binary outcomes of "Y" and "N". 

However there are 12054 missing values in pha and 3539 in neo, and these observations (rows) are being removed.

```{r message=FALSE, warning=FALSE, include=TRUE}
#check for missing values
cat("pha missing values: ", sum(is.na(data$pha)),
    "\nneo missing values : ", sum(is.na(data$neo)))
```


**The** $neo\_class$ **will have three possible values, which are defined as:**

$$
PHO \rightarrow Potential\ Hazard
$$


$$
NEO \rightarrow Near \ Earth \ Object \\ (not \ Potential \ Hazard)
$$


$$
SO \rightarrow Other \ (Solar) \ Object \\ ( not \ Near \ Earth \ object \ \& \ not \ Potential \ Hazard)
$$


Setting up the Outcome Variable :

```{r message=FALSE, warning=FALSE, include=TRUE}
#This is the outcome variable
data$pha <- as.character(data$pha)
data$neo <- as.character(data$neo)
#remove the missing values.
data <- data[complete.cases(data$neo),]
data <- data[complete.cases(data$pha),]
#convert character to numeric, 
data$pha <- ifelse(data$pha == "Y",1,0)
data$neo <- ifelse(data$neo == "Y",1,0)
#define the classes
data <- data %>%
  mutate(neo_class = as.factor(ifelse(pha == 1,"PHO",
                            ifelse(pha == 0 & neo == 1,"NEO", 
                                   ifelse(pha == 0 & pha == 0,"SO","Error")))))
describe(data$neo_class)
```

+   With only the complete cases of the flags being used, there is no missing data of the outcome variable.

+   The proportions of the PHO and the NEO are very low in the dataset, with 97.6% of the objects being classified as SO.


## Missing data

Variables with missing data:

```{r message=FALSE, warning=FALSE, include=TRUE}
#list features with less than 100% data
m_chk <- tibble()
data_f <- data.frame(data)
for(iter in 1:(ncol(data_f))){
  x <- sum(is.na(data_f[,iter]))
  cname <- as.character(colnames(data_f[iter]))
  uni <- as.character(n_distinct(data_f[iter]))
  perc <- round((((dim(data)[1]-x)/(dim(data_f)[1]))*100),1)
  if (x > 0) {
    m_chk <- bind_rows(m_chk,data_frame(Variable=cname,
                                    Distinct=uni,
                                    Missing=as.character(x),
                                    percentage=str_c(perc,"%")))}
  
}
m_chk %>% knitr::kable()
```
```{r message=FALSE, warning=FALSE, include=FALSE}
rm(m_chk,x,uni,perc,iter,cname,data_f)
```

+   $H,G,diameter,albedo,rot\_per$ are properties of the object, and useful in building a model. But besides H, the remaining variables have a large number of missing values and hence will not be used.

The observations with missing data, for the variables intended to be used for the model, are removed.

```{r message=FALSE, warning=FALSE, include=TRUE}
dim(data)
```

\newpage

## Model Variables

The original dataset, post cleaning, is now split into two datasets

+   The model Variables
+   The descriptive dataset ( _details descriptive of the objects_ )


```{r message=FALSE, warning=FALSE, include=TRUE}
#descriptive and model variables dataset separated
model_variables <- data %>%  select(id,class,neo_class,H,e,t_jup,
                                     a,q,ad,moid,moid_jup,i,om,w,per,n,ma,rms)

descriptive_data_full <- data %>% select(setdiff(names(data),names(model_variables)))
descriptive_data_full <- bind_cols(id=data$id,moid=data$moid,descriptive_data_full)
#data.table::fwrite(descriptive_data,'descriptive_data_full.csv')
descriptive_data <- descriptive_data_full %>%
  select(id,full_name,producer,neo,pha,per_y,moid,first_obs,last_obs)
data.table::fwrite(descriptive_data,'descriptive_data.csv')
#use only the complete cases. drop the remaining.
model_variables <- model_variables[complete.cases(model_variables),]
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(descriptive_data,data,descriptive_data_full,dl)
```

To make the classification model the **model_variables**  will be used.

The list of the columns in the dataset are :

```{r message=FALSE, warning=FALSE, include=TRUE}
# description of the features to be used in the model
x <- ifelse(c(desc_d$column_name) %in% c(names(model_variables)),1,0)
m_chk <- tibble()
for(iter in 1:59){  if (x[iter]==1) {
  m_chk <- bind_rows(m_chk,data_frame(Variable=desc_d[iter,2], 
                                      Description=desc_d[iter,3],
                                      Units=desc_d[iter,4]))}}
m_chk %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(iter,m_chk,x)
```

```{r message=FALSE, warning=FALSE, include=TRUE}
dim(model_variables)
```

There are 1064993 observations (rows) across 18 columns (including the Outcome Variable) in the dataset to be used for making the Classification Model.

\newpage

## Validation and Model datasets

```{r message=FALSE, warning=FALSE, include=TRUE}
# Validation set will be 10% of model_variables data
set.seed(131825431, sample.kind = "Rounding")

validation_index <- createDataPartition(model_variables$neo_class,
                                        times = 1, p = 0.1, list = FALSE)

JPL_data <- model_variables[-validation_index,]
JPL_validation <- model_variables[validation_index,]
```


```{r message=FALSE, warning=FALSE, include=TRUE}
dim(JPL_data)
```

Model dataset has 851993 observations (rows) across 18 columns (including the predictor).


```{r message=FALSE, warning=FALSE, include=TRUE}
dim(JPL_validation)
```

Validation dataset has  213000 observations (rows) across 18 columns (including the predictor).

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(model_variables, validation_index,desc_d)
```


\newpage

## Variable Summary

### id, class, neo_class

+   $id$            :   The unique id for each object.

+   $class$         :   classification of the objects accroding to their orbits.

+   $neo\_class$     :   The model Outcome Variable.


```{r message=FALSE, warning=FALSE, include=TRUE}
JPL_SmallBody_data <- data.frame(JPL_data)
summary(JPL_SmallBody_data[1:3])
```

The table below shows the distribution of the Outcome Variable by each orbital class.

```{r message=FALSE, warning=FALSE, include=TRUE}
tab <- table(JPL_SmallBody_data[2:3])
tab %>% knitr::kable()
```


A visual representation of the differrent class by the neo_class.

```{r message=FALSE, warning=FALSE, include=TRUE}
tab %>% as.data.frame() %>%
  ggplot(aes(class,Freq)) +
  geom_bar(stat="identity", fill = 9, alpha=0.5) +
  facet_wrap(~neo_class, ncol=3,scales = "free") +
  theme(axis.text.x = element_text(angle=90,size=8, hjust=1),
        axis.text.y = element_text(size=8, lineheight = 1.5),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "bottom")
```

+   The PHO and NEO have evidently a more similar orbit as compared to the SO. It might be possible to categorise our solar objects into these categories based on the observations that are available. 

However, the "class" variable will not be used for further analysis.

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(tab)
```

\newpage

### H, e, t_jup, a

+   $H$       :   Absolute Magnitude (M) is a measure of the luminosity of a celestial object, on an inverse logarithmic astronomical magnitude scale. In other words, the magnitude of an asteroid at zero phase angle and at unit heliocentric and geocentric. The more luminous an object, the smaller the numerical value of its absolute magnitude.

+   $e$       :   Eccentricity $e$ is the ratio of half the distance between the foci $c$ to the semi-major axis $e = \frac{c}{2\alpha}$. An orbit with $e=0$ is circular, $e=1$ is parabolic and $e$ between $0$ and $1$ is elliptic.

+   $t\_jup$   :   Tisserand Invariant is a value calculated from several orbital elements(semi-major axis, orbital eccentricity and inclination) of a relatively small object and a larger perturbing body, in this case Jupiter. This is used to distinguish asteroids (typically $T_J > 3$) from Jupiter-family comets (typically $2 < T_J < 3 $) amd the minor planet groups of damocloids (typically $T_J  \le 2$).

+   $a$           :   Semi-Major Axis of an orbit ellipse is half the length of the major axis. For solor system bodies, this value is denoted by $\alpha$

**Summary**
```{r message=FALSE, warning=FALSE, include=TRUE}
k <- 24576
summary(JPL_SmallBody_data[4:7])
```

+   The range of $H$ is between -1.11 (more luminous) and ~100 (less luminous).
+   The range of $e$, as expected, is between 0 and 1.
+   The range of $t\_jup$ is between -4.90 (Damocloids)  and 10.19 (Asteroids).

\newpage

```{r message=FALSE, warning=FALSE, include=TRUE }
par(mfrow=c(1,4))
for(iter in 4:7) {
  hist(JPL_SmallBody_data[,iter], main=names(JPL_SmallBody_data)[iter],xlab='', ylab='')}
```

Variations of $H, \ e, \ t\_jup$ by the $neo\_class$ (Outcume Variable) :

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
X <- JPL_SmallBody_data[,3:7]
for(iter in 2:5) {dat <- list(PHO = X[X$neo_class == "PHO",iter],
                              NEO = X[X$neo_class == "NEO",iter],SO = X[X$neo_class == "SO",iter])
ifelse(iter != 5, boxplot(dat,xlab="neo_class", cex=0, main=str_c(names(X)[iter], "")),
boxplot(dat,xlab="neo_class", log="y", cex=0, main=str_c(names(X)[5]," (log)")))}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(X,dat,iter)
```

Observations:

+   $H$ , $e$ and $t\_jup$ show some distinction (given a few outliers within) between the three prediction classes (PHO, NEO, SO).
+   $a$ shows distinction between the *SO* and the *NEO,PHO* groups in terms of range.


The stripchart below shows the spread of $H, e, t\_jup$ (of ~25,000 observations) in the $neo\_class$ :

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
X <- JPL_SmallBody_data[1:k,3:7]
for(iter in 2:5) { dat <- list(PHO = X[X$neo_class == "PHO",iter],
                              NEO = X[X$neo_class == "NEO",iter],SO = X[X$neo_class == "SO",iter])
stripchart(dat,vertical=TRUE,method="jitter", pch=16, col=1, main=str_c(names(X)[iter], ""))}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(X,dat,iter)
```
Observations: _(~25,000 observations)_

+   $H$ has a smaller range in *NEO,PHO* groups as compared to *SO*
+   $a$ has a lower range in *NEO,PHO* groups as compared to *SO*

\newpage

### q, ad, moid, moid_jup

+   $q$           :   Perihelion Distance is the distance between the orbiting body and the sun at its closest apporach.

+   $ad$          :   Aphelion Distance is the distance between the orbiting body and the sun when it is furthest away.

+   $moid$        :   Earth Minimum Orbit Intersection Distance. It is the distance between the closest point of osculating orbits of two bodies, in this case with Earth and the other small body object.

+   $moid\_jup$    :  Jupiter Minimum Orbit Intersection Distance. It is the distance between the closest point of osculating orbits of two bodies, in this case Jupiter and the other small body object.


These variables are in au (astronomical unit) : $1 \ au = 148,597,870.7 \ km$


```{r message=FALSE, warning=FALSE, include=TRUE}
summary(JPL_SmallBody_data[8:11])
```

+   The range of $a, q, ad, moid, moid\_jup$ show the spread of the distance.

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
for(iter in 8:11) { hist(log(JPL_SmallBody_data[,iter]),
                         main=str_c(names(JPL_SmallBody_data)[iter],"(log)"),xlab='', ylab='')}
```

+   The distribution chart shows that $a, q, ad, moid, moid\_jup$ have a very large number of values close to zero.   _(PS: 0.01 au is still over a million kilometers)_

\includegraphics{two.jpg}

\hfill _image source: wikipedia_

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
X <- JPL_SmallBody_data %>% select(neo_class,q,ad,moid,moid_jup)
for(iter in 2:5) {  dat <- list(PHO = X[X$neo_class == "PHO",iter],
                                NEO = X[X$neo_class == "NEO",iter],SO = X[X$neo_class == "SO",iter])
boxplot(dat,xlab="neo_class",log='y',cex=0,  main=str_c(names(X)[iter], " (log)"))}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(X,dat,iter)
```

Observations:

+   $q$ does show distinction between the *SO* and the *NEO,PHO* groups in value terms, with those in *SO* having higher values.
+   $ad$ shows distinction between the *SO* and the *NEO,PHO* groups in terms of range.
+   $moid$ shows some distinction between *SO* and *NEO,PHO* in value terms, with those in *SO* having higher $moid$.

The stripchart below shows the spread of $a, q, ad, moid, moid\_jup$ (of ~25,000 observations) in the $neo\_class$ :

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
X <- JPL_SmallBody_data %>% select(neo_class,q,ad,moid,moid_jup)
X <- X[1:k,]
for(iter in 2:5) {  dat <- list(PHO = X[X$neo_class == "PHO",iter],
                                NEO = X[X$neo_class == "NEO",iter],SO = X[X$neo_class == "SO",iter])
stripchart(dat,vertical=TRUE,method="jitter", pch=16, col=1, main=str_c(names(X)[iter], ""))}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(X,dat,iter)
```

Observations: _(~25,000 observations)_

+   $q, ad, moid$ have lower range in *NEO,PHO* groups as compared to *SO*

**Derived Variables**

A derived variables from $moid,moid\_jup$ is defined as:

+   $m\_d$ is the difference of  **Earth Minimum Orbit Intersection Distance** ($moid$) and **Jupiter Minimum Orbit Intersection Distance** ($moid\_jup$).
$$
m\_d \ = \ moid\_jup \ - \ moid
$$


**The new variable will reflect in the [Process Data] chapter.**

\newpage

### i, om, w

+   $i$       :   Inclination is the angle between the vectors normal to the object orbit plane and the specified reference plane (typically elliptic and the equatorial plane).

+   $om$      :   Longitude of the ascending node is the angle between the X-direction (typically the vernal equinox) and the point at which the body passes up (north) through the reference plane. Often denoted by $\Omega$.

+   $w$       :   Argument of perihelion is the angle (in the object orbit plane) between the ascending node line and perihelion measured in the direction of the object orbit. Often denoted by $\omega$.
                
These variables are in degrees : $\pi \ rad = \ 180 \ degrees$

```{r message=FALSE, warning=FALSE, include=TRUE}
summary(JPL_SmallBody_data[12:14]*pi/180)
```

+   $i$ has a smaller range (lower maximum value) than $om,w$.

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(3,1))
for(iter in 12:14) { 
  hist((JPL_SmallBody_data[,iter]*pi/180),
       main=str_c(names(JPL_SmallBody_data)[iter]," (radians)"), xlab='', ylab='')}
```

+   $i$ is mostly distributed in the  ~0.5 to 0 range.

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,3))
qqnorm(log2(JPL_SmallBody_data[1:k,12]*pi/180), 
       main=str_c(names(JPL_SmallBody_data)[12]," (log)"))
qqline(log2(JPL_SmallBody_data[1:k,12]*pi/180))
for(iter in 13:14) {
  qqnorm((JPL_SmallBody_data[1:k,iter]*pi/180), main=names(JPL_SmallBody_data)[iter])
  qqline((JPL_SmallBody_data[1:k,iter]*pi/180))}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(iter)
```

**Derived Variables**

+   **Inclination** ($i$),  **Longitude of the ascending node** ($om$) and **Argument of perihelion** ($w$) is being converted to radians

$$
\{ i  =  i  *  \frac{pi}{180} \} \mbox{  ,  }
\{ om  =  om  *  \frac{pi}{180} \} \mbox{  ,  }
\{ w  =  w  *  \frac{pi}{180} \}
$$

*These changes will reflect in the [Process Data] chapter.*

\newpage

### per, n, ma, rms

+   $per$     :   Orbit period is the time required for an object to make a complete revolution along its orbit. A typical main belt asteroid has an orbit period of 4 years. Orbit period in earth days is used here.

+   $n$     :   Mean Motion is the angular speed required for a body to make one orbit around an ideal ellipse with a specific semi-major axis. $\frac{(2*pi)}{Oribital \ period}$

+   $ma$    :   Mean Anomaly is the product of an orbiting object mean motion and time past the perihelion passage.

+   $rms$     :   Normalized RMS Of Orbit Fit.

```{r message=FALSE, warning=FALSE, include=TRUE}
summary(JPL_SmallBody_data[15:18])
```

+   $rms$ value rises after the 3rd quartile, (might be an outlier.)
+   $per$ value rises after the 3rd quartile, (might be an outlier.)

Outliers for $rms$ : 

```{r message=FALSE, warning=FALSE, include=TRUE}
JPL_SmallBody_data[JPL_SmallBody_data$rms > 100, 18]
```

Outliers for $per$ : 

```{r message=FALSE, warning=FALSE, include=TRUE}
table((JPL_SmallBody_data[JPL_SmallBody_data$per > (1000 *365), 3]))
```

There is are *111* SmallBody Objects with an orbit period of *greater than 10* $Centuries$, and one of them is also a NEO! (just for curiosity, which one?)

```{r message=FALSE, warning=FALSE, include=TRUE}
X <- (JPL_SmallBody_data[JPL_SmallBody_data$per > (1000 *365) & 
                            JPL_SmallBody_data$neo_class == "NEO",1])
descriptive_data <- as.data.frame(read_csv('descriptive_data.csv'))
descriptive_data %>% filter(id==X) %>% knitr::kable()
```
```{r message=FALSE, warning=FALSE, include=TRUE}
JPL_data[JPL_data$id == X, 1:11]  %>% knitr::kable()
```

Interestingly, *2017UR52* has an Earth Minimum Orbit Intersection Distance ($moid$) of $0.31949  \ au$ and has an Aphelion distance $ad$ of $643.41 \ au$.

$\rightarrow$ As a general rule, the outlier observations are not kept in the dataset, but since there are more variables the outliers of $per > 10 \ centuries$ have been kept as other variables might be able to explain the outcome. Also, even a small body with a huge (in our time frames) orbital period might be a potential NEO (as in the case of *2017UR52*) or a PHO.

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(X,descriptive_data)
```

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
for(iter in 15:18) {
  hist(log(JPL_SmallBody_data[,iter]), main=names(JPL_SmallBody_data)[iter],xlab='', ylab='')}
```


```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
X <- JPL_SmallBody_data %>% filter(rms <= 100, per <= 1000*365) %>%
  select(neo_class,per, n, ma, rms)
for(iter in 2:5) { 
  dat <- list(PHO = X[X$neo_class == "PHO",iter],
              NEO = X[X$neo_class == "NEO",iter],SO = X[X$neo_class == "SO",iter])
boxplot(dat,xlab="neo_class",log='y',cex=0,  main=str_c(names(X)[iter]," (log)"))}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(X,dat,iter)
```

Observations:

+   $per$ for most *NEO,PHO* objects are smaller than *SO* and might imply that lower the orbit period ($per$) less the likelihood of the object being and *NEO,PHO*.

The stripchart below shows the spread of $per, n, ma, rms$ (of ~25,000 observations) in the $neo\_class$ :

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,4))
X <- JPL_SmallBody_data %>% select(neo_class,per, n, ma, rms)
X <- X[1:k,]
for(iter in 2:5) {
  dat <- list(PHO = X[X$neo_class == "PHO",iter],
              NEO = X[X$neo_class == "NEO",iter],SO = X[X$neo_class == "SO",iter])
  stripchart(dat,vertical=TRUE,method="jitter", pch=16, col=1, main=str_c(names(X)[iter], ""))}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(X,dat,iter,k)
```

**Derived Variables**

+   **Time past the perihelion passage** $t_{perihelion}$ can be derived from **Mean Anamoly** ($ma$) and **Mean Motion**. ( _reference to time is in days_ )

$$
t_{perihelion} =  \frac{ma}{n}
$$

+    **Time past the Aphelion passage** ($t_{aphelion}$) is the difference between **Orbital period** ($per$) and **Time past the perihelion passage** ($t_p$). ( _reference to time is in days_ )

$$
per  =  t_{aphelion}  +   t_{perihelion} 
$$


$$
t_{perihelion}  =  per  -   t_{aphelion} 
\ \mbox{  or  } \ 
t_{aphelion}  =  per  -  \frac{ma}{n}
$$

+   **Ratio of perihelion period** ($T_p$) can thus be derived as 


$$
T_p = \frac{t_{perihelion}}{per}
$$
$$
=>  T_{p}  =  1 - \frac{t_{aphelion}}{per} \ \mbox{  or  } \ T_p = \frac{\frac{ma}{n}}{per}
$$

**These new variables will reflect in the [Process Data] chapter.**


\newpage


## Variable Means

Comparison of variable means by $neo\_class$ :

```{r message=FALSE, warning=FALSE, include=TRUE}
pp <- JPL_SmallBody_data[,3:18]
m_comp <- data_frame()
for(iter in 2:15){
   Mean_PHO <- round(mean(pp[pp$neo_class=="PHO",iter]),2)
   Mean_NEO <- round(mean(pp[pp$neo_class=="NEO",iter]),2)
   Mean_SO <- round(mean(pp[pp$neo_class=="SO",iter]),2)
   m_comp <- bind_rows(m_comp, 
                       data_frame(Variable=colnames(pp[iter]),
                                  PHO = Mean_PHO,NEO = Mean_NEO,SO = Mean_SO))}
m_comp  %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(pp,m_comp,iter,Mean_PHO,Mean_NEO,Mean_SO)
```

Observations:

+   $per$ group means in descending order -  *SO,NEO,PHO*.
+   $ma$ group means in descending order -  *PHO,SO,NEO*.
+   $H$ group means in descending order -  *NEO,PHO,SO*.
+   $q$ group means in descending order -  *SO,NEO,PHO*.
+   $i$ group means in descending order -  *PHO,NEO,SO*.

These observations show that the variables display a pattern in their values.

\newpage

## Distance

_But, a picture is worth a thousand words._

The JPL_SmallBody_data dataset is normalized by columns, for each $neo\_class$, and then ordered according to the results of the clustering algorithm for a visual cue.

```{r message=FALSE, warning=FALSE, include=TRUE, fig.height=6}
x <- JPL_SmallBody_data[,3:18]

#extract and sweep different categories
ph <- x[x$neo_class =="PHO",-1] %>% as.matrix() %>% sweep(.,2, colMeans(.))
ne <- x[x$neo_class =="NEO",-1] %>% as.matrix() %>% sweep(.,2, colMeans(.))
so <- x[x$neo_class =="SO",-1] %>% as.matrix() %>% sweep(.,2, colMeans(.))

#sweep the data matrix
x <-  as.matrix(x[,-1])
x <- sweep(x,2, colMeans(x))

k <- 42
#hclusters based on data and the transpose
ph_1 <- hclust(dist(ph[1:k,]))
ph_2 <- hclust(dist(t(ph[1:k,])))
ne_1 <- hclust(dist(ne[1:k,]))
ne_2 <- hclust(dist(t(ne[1:k,])))
so_1 <- hclust(dist(so[1:k,]))
so_2 <- hclust(dist(t(so[1:k,])))

#image of the hclust
par(mfrow=c(1,3))
image(x[ph_1$order, ph_2$order],
  col=RColorBrewer::brewer.pal(11,'RdBu'), main = "Potential Hazard")

image(x[ne_1$order, ne_2$order],
  col=RColorBrewer::brewer.pal(11,'RdBu'), main = "Near Earth")

image(x[so_1$order, so_2$order],
  col=RColorBrewer::brewer.pal(11,'RdBu'), main = "Solar Object")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(ph_1,ph_2,ne_1,ne_2,so_1,so_2,ph,ne,so,x,k,JPL_SmallBody_data)
gc()
```

Observations:

+   The image gives a clear visual cue on how the variables in the data provide a clear distinction between the individual categories of the Outcome variable $neo\_class$.

\newpage

# Train & Test sets

```{r message=FALSE, warning=FALSE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
# test set will be 10% of training set.
test_index <- createDataPartition(JPL_data$neo_class, times = 1, p = 0.1, list = FALSE)
JPL_train <- JPL_data[-test_index,]
JPL_test <- JPL_data[-test_index,]
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(test_index)
```

## Process Data

The variables are modified as mentioned above for the datasets to be used for training and testing. This data processing will also be done for the validation set.

```{r message=FALSE, warning=FALSE, include=TRUE}
data_process <- function(preData, tChoice){
  #convert to radians and add additional derived variables
    preData <- preData %>%
    mutate(om=om*(pi/180), w=w*(pi/180), i=as.numeric(i*(pi/180)),
           t_to_p = as.numeric((per - (ma/n))), a=log2(a),
           m_d = as.numeric(moid_jup-moid))
    preData <- preData %>% mutate(T_p = as.numeric(round(1-(t_to_p/per),4)))
    preData <- preData %>% select(neo_class,H,q,moid,m_d,e,t_jup,a,i,T_p)
    preData$neo_class <- as.factor(preData$neo_class)
    
    #return as matrix format
    if(tChoice == 1){
      preData <- data.frame(preData)
      x_matrix <- as.matrix(preData[,-1])
      y_matrix <- factor(preData[,1])
      processData <- list(X = x_matrix, Y = y_matrix)}
    #return as data table
    if(tChoice == 2) processData <- preData
    return (processData)
}
```

## Inaccuracy Details function

```{r message=FALSE, warning=FALSE, include=TRUE, results='hide'}
descriptive_data <- data.table::fread("descriptive_data.csv", na.strings = c(NA,"NA",""))
descriptive_data <- as.data.frame(descriptive_data)

#function to collate inaccurately classified observations.
inaccurate_details <- tibble()
inaccuracy_check <- function(incorrect_p,method,dat){
  idetails <- data.frame()
  for(each in incorrect_p){ 
    a <- (dat[each,1])
    idetails <- rbind(idetails,data.frame(model_name = method,
                                        descriptive_data[descriptive_data$id==str_trim(a),]))}
  return(idetails)}
```

\newpage

# Model Evaluations

## Evaluation metrics

### Definations

**The confusion matrix**


|                      | **Actual Positive** | **Actual Negative** |
|----------------------|---------------------|---------------------|
|**Predicted Positive**| True Positive (TP)  | False Positive (FP) |
|**Predicted Negative**| False Negative (FN) | True Negative (TN)  |


**Recall, 1-FPR, and Precision**

|  |  |  | Definition | Probability representation |
|---------|-----|----------|--------|------------------|
sensitivity | TPR | Recall | $\frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}$ | $\mbox{Pr}(\hat{Y}=1 \mid Y=1)$ |
specificity | TNR | 1-FPR | $\frac{\mbox{TN}}{\mbox{TN}+\mbox{FP}}$ | $\mbox{Pr}(\hat{Y}=0 \mid Y=0)$ |
specificity |  PPV | Precision | $\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}$ | $\mbox{Pr}(Y=1 \mid \hat{Y}=1)$|

**F1 Score**

This is the harmonic average of Precision and Recall and is given as :

$$
2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
$$


### Evaluation Function

In this case study the confusionmatrix function of the caret package is used. The following function takes the confusion matrix and adds the Recall and the F1 scores to a tibble for the results.

```{r message=FALSE, warning=FALSE, include=TRUE}
#function to collate the results
results_process<- function(x,name){
  x1 <- data_frame(model_name=name,Object="NEO",
                   Recall=round(x[1,6],3),F1=round(x[1,7],3))
  x2 <- data_frame(model_name=name,Object="PHO",
                   Recall=round(x[2,6],3),F1=round(x[2,7],3))
  x3 <- data_frame(model_name=name,Object="SO",
                   Recall=round(x[3,6],3),F1=round(x[3,7],3))
  return(bind_rows(x1,x2,x3))
}
```

\newpage

## PCA

The principal component Analysis assists in reducing the complexity of model being fit by transforming the data, with the distance between the rows preserved but with the variance of the variables in decreasing order.

```{r message=FALSE, warning=FALSE, include=TRUE}
mat_train <- data_process(JPL_train,1)
mat_test <- data_process(JPL_test,1)
set.seed(131825431, sample.kind = "Rounding")
pca <- prcomp(mat_train$X, center = TRUE, scale. = TRUE)
summary(pca)
```

+   The first six principal components account for about 80% of the variance in our training set.

+   The plot below shows the corresponding variances for the PCAs.

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,1))
screeplot(pca,type = "line", main="PCA")
```

\newpage

Correlation Plot shows the correlation between the variables.

```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,2))
corrplot(cor(mat_train$X), method="circle", type="lower")
corrplot(cor(pca$x), method="circle", type="upper")
```

Observations:

+   The $qa\_d$ variable, which is the differnce between the Perihelion Distance $q$ and the Aphelion Distance $ad$ has a correlation of 0.92 with the Semi-Major Axis $a$.
+   There is no corelation in the pca components.

\newpage


Plot of the the first 4 components of the PCA together even on a random sample, gives a visual cue on the information on the $neo\_class$. 

**Note:** random sample of 24,576 observations used.

```{r message=FALSE, warning=FALSE, include=TRUE, cache=TRUE}
set.seed(131825431, sample.kind = "Rounding")
par(mfrow=c(1,1))
p1 <- data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], label= mat_train$Y) %>%
  sample_n(24576) %>%  ggplot(aes(PC1,PC2, fill = label)) + 
  geom_point(cex=3,pch=21,alpha=0.7) + theme(legend.position = "right")
p2 <- data.frame(PC3 = pca$x[,3], PC4 = pca$x[,4],label= mat_train$Y) %>%
  sample_n(24576) %>% ggplot(aes(PC3,PC4, fill = label)) + 
  geom_point(cex=3,pch=21,alpha=0.7) + theme(legend.position = "hide")
gridExtra::grid.arrange(p1,p2, ncol=2, nrow=1)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(p1,p2)
gc()
```

\newpage

### Multinom

The first model that is tried is a multinom model using the nnet library. It fits multinomial log-linear models via neural networks.

+   The first 7 components of the PCA are used, which account for 98.6% of the variance.

+   The test set is rotated using the rotation matrix of PCA.

Model Fitting : 
```{r fitmultinom, message=FALSE, warning=FALSE, include=TRUE, cache=TRUE, results='hide'}
#first 7 components
k <- 7
train_pca <- data.frame(pca$x[,1:k], neo_class= as.factor(mat_train$Y))
test_pca <- as.matrix(sweep(mat_test$X,2, colMeans(mat_test$X))) %*% pca$rotation
test_pca <- data.frame(test_pca[,1:k], neo_class= as.factor(mat_test$Y))
set.seed(131825431, sample.kind = "Rounding")
fit_pca_multinom <- multinom(neo_class ~ ., data = train_pca, sumn=1)
```

Model Prediction and the confusion matrix: 
```{r message=FALSE, warning=FALSE, include=TRUE}
y_hat_pca_multinom <- predict(fit_pca_multinom,test_pca, type = "class")
cm <- confusionMatrix(y_hat_pca_multinom, test_pca$neo_class)
results <- results_process(as.data.frame(cm$byClass),"PCA Multinom")
cm$table
```

```{r message=FALSE, warning=FALSE, include=TRUE}
cat("Accuracy : ", cm$overall["Accuracy"], "\n")
```

+   The multinom model was able to identify some of the categories, however since the 7 PCA components used explain about 99.7% of the variablily. The accuracy that we have is almost 98%, but it is the Sensitivity in individual categories (SO has a Recall of 1) that matters as well in this case.

```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "PCA Multinom")  %>% knitr::kable()
```

The PCA components, though explain the variablitly with lesser variables (components) will not be used as the Recall of 0.99 is the objective, and though accuracy is high the requiremnt of Recall is not being met.

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(pca,fit_pca_multinom,k,test_pca,train_pca,y_hat_pca_multinom,cm)
gc()
```

\newpage

## Naive Bayes

The second model is the Naive Bayes model, a general genrative model.

+   In a binary case, the smallest true error ahchieved is determined by the Bayes rule, which is a decision rule that is based on true conditional probabilty, and is given by $p(X)=Pr(Y=1|X=x)$.

+   Predictors are assumed to be independent within each class label.

+   Numeric (metric) predictors are handled by assuming that they follow Gaussian distribution, given the class label, and are modelled with Poisson distribution.

+   Prediction is done by the type class.

Model Fitting, Prediction and the confusion matrix:
```{r message=FALSE, warning=FALSE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
fit_nb <- naive_bayes(mat_train$X,mat_train$Y, usekernel=FALSE, usepoisson = TRUE)
y_hat_nb <- predict(fit_nb, mat_test$X, type = "class")
cm <- confusionMatrix(y_hat_nb, mat_test$Y)
results <-  bind_rows(results,results_process(as.data.frame(cm$byClass),"Naive Bayes"))
cm$table
```

+   This model does a bit with a higher F1 and Recall and is able to classify NEO and PHO better.

```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "Naive Bayes")  %>% knitr::kable()
```

+   The Recall for all three categories is above 0.9 but the F1 Score (the harmonic average of Precision and Recall) for PHO is below 0.5

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(fit_nb,y_hat_nb,cm)
gc()
```


\newpage

## Ensemble

In order to get the right direction on the model which will fit well with the data set, an Ensemble of 6 models is run on a smaller sample.

+   LDA : Linear discriminant Analysis. The assumption here is correlation structure is the same for all classes, which thus reduces the number of parameters that are required to estimate.

+   QDA : Quadratic discriminant Analysis. This is a version of Naive Bayes in which the assumption is that the distributions $p_{X|Y=1}(x)$ and $p_{X|Y=0}(x)$ are multivariate normal.

+   KNN : k-Nearest Neighbor Classification. The k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote.

+   rpart : Recursive Partitioning and Regression Trees. These are decision trees that work by predicting an outcome variable $Y$ by partitioning the predictors.

+   svmRadialCost : Support Vector Machine with radial cost. In simple terms, the SVM consturcts a linear model with the largest possible margin given to the data points.

+   wsrf : Forest of Weighted Subspace Decision Trees. C4.5-based trees (Quinlan (1993)) are grown by wsrf, where binary split is used for continuous predictors (variables) and k-way split for categorical ones.


**Note: ** _sample of 16,384 observations for training and 1638 obeservations for testing is used._

Model Fitting :
```{r ensemblefit, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE, results='hide'}
set.seed(131825431, sample.kind = "Rounding")
ensemble_train <- data_process(JPL_train[1:16348,],1)
ensemble_test <- data_process(JPL_test[1:1638,],1)
results_ensemble <- data.frame()

models <- c("lda", "qda", "knn", "rpart", "svmRadialCost", "wsrf")

fits <- lapply(models, function(model){
  cat(":- ",model, "\n")
  if (model %in% c("lda","qda")){
    train(ensemble_train$X,ensemble_train$Y, method=model, prior = c(1,1,1)/3)
  }
  else train(ensemble_train$X,ensemble_train$Y, method=model,
             trControl = trainControl(method = "cv", number=5))
}) 
```

\newpage

Prediction and the confusion matrix :  
```{r message=FALSE, warning=FALSE, include=TRUE}
names(fits) <- models
pred <- sapply(fits,function(object){  predict(object, ensemble_test$X) })
  
for (each in 1:ncol(pred)){
  cm <- confusionMatrix(factor(pred[,each]), ensemble_test$Y)
  results_ensemble <- bind_rows(results_ensemble,
                                results_process(as.data.frame(cm$byClass),colnames(pred)[each]))
}
```


Ensemble Results:

```{r message=FALSE, warning=FALSE, include=TRUE}
results_ensemble %>% filter (model_name == "lda")  %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=TRUE}
results_ensemble %>% filter (model_name == "qda")  %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=TRUE}
results_ensemble %>% filter (model_name == "knn")  %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=TRUE}
results_ensemble %>% filter (model_name == "rpart")  %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=TRUE}
results_ensemble %>% filter (model_name == "svmRadialCost")  %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=TRUE}
results_ensemble %>% filter (model_name == "wsrf")  %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(ensemble_test,ensemble_train,fits,models,pred,results_ensemble,cm,each)
gc()
```

+   The qda model has a Recall of 1 for the PHO category.

+   The rpart and the wsrf models fit well with this subset of the data. 

Thus, from the ensemble it seems that the decision tree based models will be the ones which achieve the objective of 0.99 Recall and 0.99 F1.


\newpage

## Decision Trees

### Rpart

Recursive Partitioning and Regression Trees. 

These are decision trees that predict an outcome variable $Y$ by partitioning the predictors.

The idea is to build a decision tree and, at the end of each node, obtain a predictor $\hat{y}$. The prediction space is partitioned into $J$ non-overlapping regions, $R_1,R_2,...,R_J$ and then for any predictor $x$ that falls within the region $R_j$,  estimate $f(x)$ with the average of the training observations $y_i$ for which the associated predictor $x_i$ is also in $R_j$. Regression trees create partitions recursively. Description of how the partition is picked for further partitions and when to stop is given below:

$$
R_1(j,s) = \{x|x_j < s \} \mbox{  and  } R_2(j,s) = \{x|x_j \ge s \}
$$

+   $x$ is the partition to be split.
+   $j$ is the predictor.
+   $s$ is the value that defines the 2 partitions.

$j$ and $s$  are picked up by finding the pair that minimizes the residual sum of squares (RSS):
$$
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
$$

This is then applied recursively to the new regions $R_1$ and $R_2$. After partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.


**Model Parameters : **

+   Complexity Parameter : The residual sum of squares must improve by a factor of cp for the new partition to be added. Large values of cp will therefore force the algorithm to stop earlier which results in fewer nodes.


Complexity Parameter :

```{r fitrpart, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
train_rpart <- train(mat_train$X,mat_train$Y, method="rpart",
                     tuneGrid = data.frame(cp=seq(0.0,0.05, len=25)),
                     trControl = trainControl(method = "cv", number=5))
rpart_cp <- train_rpart$bestTune
rpart_cp
```


Variable Importance :

```{r message=FALSE, warning=FALSE, include=TRUE}
train_rpart$finalModel$variable.importance
```

+   $moid,q,H,a,m\_d,e,i,t\_jup$ are used in the rpart fit.

Visual cue on the decision tree:

```{r message=FALSE, warning=FALSE, include=TRUE}
plot(train_rpart$finalModel, margin=0.1)
text(train_rpart$finalModel, cex=0.6)
```


Model Fitting, Prediction and the confusion matrix:
```{r ftrainrpart, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
rtrain <- data_process(JPL_train,2)
rtest <- data_process(JPL_test,2)
set.seed(131825431, sample.kind = "Rounding")
fit_rpart <- rpart(neo_class ~. , data=rtrain, parms = list(split = "gini"),
             control = rpart.control(cp = rpart_cp))
predcit_rpart <- predict(fit_rpart, rtest)
labels <- colnames(predcit_rpart)[apply(predcit_rpart, 1, which.max)]
cm <- confusionMatrix(factor(labels), rtest$neo_class)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"rpart"))
cm$table
```

Results :
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "rpart")  %>% knitr::kable()
```



+   The Recall is $\ge$ 0.99, but the F1 score for $PHO$ is 0.985

However, the rpart is known to not be very flexible and is unstable to changes in the training data. Random Forest is used next to overcome some of these shortcomings.

```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  labels != rtest$neo_class ),"rpart",JPL_test))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(train_rpart,rtrain,rtest,fit_rpart,predcit_rpart,labels,cm,incorrect_p)
gc()
```

\newpage

### Random Forest


Random forests are a popular machine learning approach that addresses the shortcomings of decision trees  and improving prediction performance and reduce instability by _averaging_ multiple decision trees (a forest of trees constructed with randomness).

This is done by

+   Bagging   :   Generate many predictors, each using regression or classification trees.
+   Bootstrap :   To induce randomness of individual trees bootstrap is used.

The individual trees **randomly** different, and the combination of trees is the **forest**. 

randomForest implements Breiman random forest algorithm (based on Breiman and Cutler original Fortran code) for classification and regression.


Tuning Function :
```{r gettunedFunction, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
gettuned <- function(method_name,grid,dat){
  set.seed(131825431, sample.kind = "Rounding")
  control <- trainControl(method='cv', number = 5)
  train(x = dat$X[2048:8192,], y = dat$Y[2048:8192], nTree=50,
        method=method_name, tuneGrid = grid, trControl = control)$bestTune}
```

**Model Parameters : **

+   mtry      :   Number of variables randomly sampled as candidates at each split. The function $gettuned$ above gets the mtry for the bestTuned randomForest of a small sample size of 6,144 observations.

+   strata    :   A (factor) variable that is used for stratified sampling.

+   sampsize  :   Size of sample to draw.

+   nodesize	:   Minimum size of terminal nodes. For classification it is 1.

+   nTree     :   Number of trees to grow.

```{r message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
rf_mtry <- gettuned("rf",data.frame(mtry=c(2,3,4,5,6)),mat_train)
```

Model Fitting : 
```{r fitrf, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
rf_train <- data_process(JPL_train,2)
rf_test <- data_process(JPL_test,2)
fit_rf <- randomForest(neo_class ~ .,data=rf_train, mtry=rf_mtry$mtry,
                       nodesize = 1, strata = neo_class, nTree=rf_mtry$mtry*50,
                       sampsize = 100000, replace=FALSE)
```


Gini Index is defined as 
$$
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
$$

+   $\hat{p}_{j,k}$ as the proportion of observations in partition 
+   $j$ that are of class $k$.

GINI importance measures the gain of purity by partitions of a given variable and assists in the decision function of the random forest to select from available partitions.

Variable Importance of the last measure:
```{r message=FALSE, warning=FALSE, include=TRUE}
varImpPlot(fit_rf)
```

+   $q,moid,H$ achieved the highest meanDecrease, in the last measure.

Prediction and the confusion matrix: 
```{r message=FALSE, warning=FALSE, include=TRUE}
y_hat_rf <- predict(fit_rf,rf_test)

cm <- confusionMatrix(y_hat_rf, rf_test$neo_class)
results <-  bind_rows(results,results_process(as.data.frame(cm$byClass),"Random Forest"))
cm$table
```

Results :
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "Random Forest")  %>% knitr::kable()
```

+   The Recall is $\ge$ 0.99, but the F1 scores for $PHO$ is 0.986

+   In the rpart NEO had 38 misses and PHO had 15 misses. In the Random Forest though the NEO misses have been lesser the PHO misses have gone up.

This might be caused due to the structure of the data which has over 97.6% of SO. The shortcoming of not being able to associate weights to categories of importance is approached with the WSRF model.

```{r message=FALSE, warning=FALSE, include=TRUE}
incorrect_p <- which(y_hat_rf != rf_test$neo_class )
inaccurate_details <- rbind(inaccurate_details,inaccuracy_check(
  incorrect_p,'Random Forest',JPL_test))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(fit_rf,rf_test,rf_train,y_hat_rf,cm)
gc()
```

\newpage

### WSRF

Forest of Weighted Subspace Decision Trees

**Model Parameters : **

+   mtry        :   number of trial predictors for a split (mtry).

+   nTree       :   the number of trees to train.

+   minNode     :   minimum number of distinct row references to split a node.

+   weights     :   TRUE for weighted subspace selection.

+   importance	:   Importance of predictors.


```{r message=FALSE, warning=FALSE, include=TRUE, cache=TRUE}
wsrf_mtry <- gettuned("wsrf",data.frame(mtry=c(2,3,4,5,6)),mat_train)
```

Model Fitting:
```{r fitwsrf, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
fit_wsrf <- wsrf(x = mat_train$X, y = mat_train$Y,
                 ntree = wsrf_mtry$mtry*100, mtry=wsrf_mtry$mtry+1,
                 weights=TRUE,  importance = TRUE, parallel = TRUE)
```

Variable Importance:
```{r message=FALSE, warning=FALSE, include=TRUE}
fit_wsrf$importance
```
Prediction and confudsion matrix :
```{r message=FALSE, warning=FALSE, include=TRUE}
y_hat_wsrf <- predict(fit_wsrf, mat_test$X)
y_hat_wsrf_m <- as.matrix(y_hat_wsrf$class)
y_hat_wsrf_m <- ifelse(y_hat_wsrf_m == 1, 'NEO',ifelse(y_hat_wsrf_m == 2, 'PHO','SO'))

cm <- confusionMatrix(as.factor(y_hat_wsrf_m),mat_test$Y)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"WSRF"))
cm$table
```

Inaccurate Predictions :
```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  mat_test$Y != y_hat_wsrf_m),"WSRF",JPL_test))
inaccurate_details %>% filter (model_name == "WSRF") %>% .[,3:10] %>% tibble() %>% knitr::kable()
```

Results : 
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "WSRF")  %>% knitr::kable()
```

+   The Recall and F1 scores are all $>$ 0.99

+   Using a model which implements weights does increase the sensitivity on the outcome prediction.

+   However there still are a total of 8 misses in PHO and 5 misses in NEO. 

This might be partially explained by the incapability to attach weights manually, only a logical value is accepted in this model. 

Thus the shortcoming of not being able to associate weights manually to each category according to their importance is approached with the Ranger model.



```{r message=FALSE, warning=FALSE, include=FALSE}
rm(y_hat_wsrf,y_hat_wsrf_m,cm)
gc()
```

\newpage

### Ranger

Ranger is a fast implementation of random forests (Breiman 2001) or recursive partitioning, particularly suited for high dimensional data. Classification and regression forests are implemented as in the original Random Forest (Breiman 2001). Includes implementations of extremely randomized trees (Geurts et al. 2006) and quantile regression forests (Meinshausen 2006).

**Model Parameters : **

+   classification is set to TRUE and accordingly min.node.size is set to 1.

+   importance      :   Variable importance mode.

+   splitrule       :   For classification the default "gini".

+   regularization.usedepth   :   The depth in regularization.

+   num.threads     :   Number of threads / number of CPUs.

+   class.weights   :   Weights for the outcome classes (in order of the factor levels) in the splitting rule (cost sensitive learning). For classification the weights are also applied in the majority vote in terminal nodes.

Model Fitting : 
```{r fitranger, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
fit_ranger <- ranger(x = mat_train$X, y = mat_train$Y, 
                     classification = TRUE, 
                     min.node.size = 1,
                     mtry=rf_mtry$mtry+1,
                     num.trees = rf_mtry$mtry*50, 
                     splitrule = 'gini',
                     importance = 'impurity', 
                     regularization.usedepth = TRUE,
                     # additional weight of ~ 40:60:0  ~ 2:3:0 ratio from prime numbers. 
                     class.weights = c(7,11,0), 
                     num.threads = 6, verbose = TRUE)
```

Variable Importance :

```{r message=FALSE, warning=FALSE, include=TRUE}
fit_ranger$variable.importance
```

\newpage

Prediction and the confusion matrix : 
```{r message=FALSE, warning=FALSE, include=TRUE}
y_hat_ranger <- predict(fit_ranger, mat_test$X)

cm <- confusionMatrix(y_hat_ranger$predictions, mat_test$Y)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"Ranger"))
cm$table
```

+   **Unity is achieved.** No misses.

```{r message=FALSE, warning=FALSE, include=TRUE}
cm$byClass
```

+   **100% accuracy on the training set**
+   Prevalance shows the ditribution of the $neo\_class$ factors as determined on the orginal data (a 97.6% for SO, 0.002 for PHO and 0.022 for NEO)

Results : 
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "Ranger")  %>% knitr::kable()
```


+   This will be one of the models that will be used for the Final validation set. 



```{r message=FALSE, warning=FALSE, include=FALSE}
rm(y_hat_ranger,cm)
gc()
```

\newpage

## Gradient Boosting

Generalized Boosted Regression Modeling (GBM)

Boosting is the process of iteratively adding basis functions in a greedy fashion so that each additional basis function further reduces the selected loss function. This implementation closely follows Friedman Gradient Boosting Machine (Friedman, 2001).

The **GBM** implements the generalized boosted modeling framework.

**Model Parameters : **

+   n.trees             :   Integer specifying the total number of trees to fit.

+   class.stratify.cv   :   Logical indicating whether or not the cross-validation should be stratified by class. The purpose of stratifying the cross-validation is to help avoiding situations in which training sets do not contain all classes.

+   bag.fraction        :   The fraction of the training set observations randomly selected to propose the next tree in the expansion. This introduces randomnesses into the model fit.

+   train.fraction      :   The first train.fraction * nrows(data) observations are used to fit the gbm and the remainder are used for computing out-of-sample estimates of the loss function.

+   shrinkage           :   A shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction.

+   keep.data           :   a logical variable indicating whether to keep the data and an index of the data stored with the object.

+   interaction.depth	  :   Integer specifying the maximum depth of each tree (i.e., the highest level of variable interactions allowed). A value of 1 implies an additive model. A value of 3 implies a model with up to 3-way interactions.

+   n.cores             :   The number of CPU cores to use. The cross-validation loop will attempt to send different CV folds off to different cores.


Fitting the Model :
```{r fitgbm, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
gbm_train <- data_process(JPL_train,2)
gbm_test <- data_process(JPL_test,2)
set.seed(131825431, sample.kind = "Rounding")
fit_gbm <- gbm(neo_class~., data = gbm_train, n.trees = 50,
               distribution = "multinomial", class.stratify.cv =TRUE,
               bag.fraction = 0.3, train.fraction = 0.7, shrinkage = 0.1, 
               interaction.depth = 3, keep.data = FALSE, verbose = FALSE, n.cores = 6)
```

Visual representation of the model:
```{r message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,1))
best.iter <- gbm.perf(fit_gbm, plot.it = FALSE)
p1 <- plot(fit_gbm)
p2 <- plot(fit_gbm, i.var = 1:2, n.trees = best.iter)
gridExtra::grid.arrange(p1,p2, ncol=2, nrow=1)
```

\hfill

Prediction and the confusion matrix:

```{r message=FALSE, warning=FALSE, include=TRUE}
y_hat_gbm <- predict(fit_gbm, gbm_test, n.trees = best.iter, type='response')
labels <- colnames(y_hat_gbm)[apply(y_hat_gbm, 1, which.max)]

cm <- confusionMatrix(factor(labels), gbm_test$neo_class)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"gbm"))
cm$table
```

\newpage

Results:
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "gbm")  %>% knitr::kable()
```

+   The misses on PHO and NEO increased.

+   The Recall and F1 scores for PHO are 0.991 and 0.986


```{r echo=FALSE, message=FALSE, warning=FALSE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  labels != gbm_test$neo_class),'gbm',JPL_test))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(gbm_train,gbm_test,p1,p2,y_hat_gbm,cm,labels)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(mat_test,mat_train,JPL_test, JPL_train)
gc()
```

\newpage

## Collated Results

### Recall


```{r echo=FALSE, message=FALSE, warning=FALSE}
F1_score <- results %>% select(model_name, F1, Object) %>% spread(.,Object,F1)
Recall_score <- results %>% select(model_name, Object, Recall) %>% spread(.,Object,Recall)
```

\hfill

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(Recall_score)
```

\hfill

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,1))
in_m <- c( "rpart","Random Forest","WSRF","gbm","Ranger")
results %>% filter(model_name %in% in_m) %>% filter(Object != "SO") %>%
  ggplot(aes(reorder(model_name,Recall), Recall)) + geom_point(pch=19, cex=3, alpha=0.7)  +
  facet_wrap(~Object, ncol=3,scales = "fixed") +
  theme(axis.text.x = element_text(angle=90,size=8, lineheight = 1.2, hjust=1),
        axis.text.y = element_text(size=8, lineheight = 1.2),
        axis.title.x = element_blank(),
        legend.position = "bottom")
```

\newpage

### F1


\hfill

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(F1_score)
```

\hfill

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,1))
in_m <- c( "rpart","Random Forest","WSRF","gbm","Ranger")
results %>% filter(model_name %in% in_m) %>% filter(Object != "SO") %>%
  ggplot(aes(reorder(model_name,F1), F1)) + geom_point(pch=19, cex=3, alpha=0.7)  +
  facet_wrap(~Object, ncol=3,scales = "fixed") +
  theme(axis.text.x = element_text(angle=90,size=8, lineheight = 1.2, hjust=1),
        axis.text.y = element_text(size=8, lineheight = 1.2),
        axis.title.x = element_blank(),
        legend.position = "bottom")
```

\newpage

### F1 vs Recall


\hfill

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,1))
results %>% filter(model_name != "PCA Multinom") %>% 
  ggplot(aes(F1,Recall, col=Object)) + geom_point(pch=19, cex=3)  +
  facet_wrap(~model_name, nrow=2, scales = "free") +
  theme(axis.text.x = element_text(angle=90,size=8, lineheight = 1.2, hjust=1),
        axis.text.y = element_text(size=8, lineheight = 1.2),legend.position = "bottom")
```

\hfill

+   The Rpart, randomForest, wsrf, ranger and gbm models have a Recall $>$ 0.99

+   Based on the scores on the predictions from the training and test sets, the final validation models will be on Ranger and WSRF as these models have both the Recall and F1 $\ge$ 0.99 for all the three categories.

+   Rpart is also tested on the validation set.



```{r message=FALSE, warning=FALSE, include=FALSE}
rm(F1_score,Recall_score)
gc()
```

\newpage

# Final Models

## Validation

### Ranger

Validation set matrix:
```{r message=FALSE, warning=FALSE, include=TRUE}
#process as matrix for the model
validation <- data_process(JPL_validation,1)
JPL_F <- data_process(JPL_data,1)
```

Model Fitting, Prediction and the confusion matrix : 
```{r Finalranger, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
#fit the model for validation
final_ranger <- ranger(x=JPL_F$X, y=JPL_F$Y,
                     classification = TRUE, min.node.size = 1,
                     num.trees = rf_mtry$mtry*50, mtry=rf_mtry$mtry+1, 
                     splitrule = 'gini', importance = 'impurity', 
                     num.threads = 6, regularization.usedepth = TRUE, 
                     verbose = FALSE, class.weights = c(7,11,0))

#predict and the confusion matrix
pred_ranger <- predict(final_ranger, validation$X)
cm <- confusionMatrix(pred_ranger$predictions, validation$Y)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"Validation : Ranger"))
cm$table
```


Results : 
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name =="Validation : Ranger")  %>% knitr::kable()
```

+   The Recall and the F1 for the Ranger model on the validation dataset, as validation is $>$ 0.99

\hfill
\hfill |
\hfill

Inaccurate Predictions :
```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  pred_ranger$predictions != validation$Y),'Validation : Ranger',JPL_validation))
inaccurate_details %>% filter (model_name == "Validation : Ranger") %>% .[,3:10] %>%
  tibble() %>% knitr::kable()
```

\newpage

###   WSRF

Model Fitting, Prediction and the confusion matrix : 
```{r finalwsrf, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
#fit the model for validation
final_wsrf <- wsrf(x=JPL_F$X, y=JPL_F$Y,
                 ntree = wsrf_mtry$mtry*100, mtry=wsrf_mtry$mtry+1,
                 weights=TRUE,  importance = TRUE, parallel = TRUE)
#predict and the confusion matrix
pred_wsrf <- predict(final_wsrf, validation$X)
pred_wsrf_m <- as.matrix(pred_wsrf$class)
pred_wsrf_m <- ifelse(pred_wsrf_m == 1, 'NEO',ifelse(pred_wsrf_m == 2, 'PHO','SO'))

cm <- confusionMatrix(as.factor(pred_wsrf_m),validation$Y)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"Validation : WSRF"))
cm$table
```

Results : 
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "Validation : WSRF")  %>% knitr::kable()
```

+   The Recall for the PHO and SO is 1, with that of NEO $>$ 0.99
+   The F1 for all the ctegories is $>$ 0.99

\hfill

Inaccurate Predictions :
```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  validation$Y != pred_wsrf_m),"Validation : WSRF",JPL_validation))
inaccurate_details %>% filter (model_name == "Validation : WSRF") %>% .[,3:10] %>%
  tibble() %>% knitr::kable()
```



### Rpart

Model Fitting, Prediction and the confusion matrix : 
```{r finalrpart, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
#process as data tables for the model
validation <- data_process(JPL_validation,2)
JPL_F <- data_process(JPL_data,2)
#fit model
Final_rpart <- rpart(neo_class ~. , data=JPL_F,
             control = rpart.control(cp = rpart_cp, minsplit = 10, minbucket=1))
#predict and confusion matrix
pred_rpart <- predict(Final_rpart, validation)
labels <- colnames(pred_rpart)[apply(pred_rpart, 1, which.max)]
cm <- confusionMatrix(factor(labels), validation$neo_class)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"Validation : rpart"))
cm$table
```

Results :
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "Validation : rpart")  %>% knitr::kable()
```

+   The Recall and the F1 for all the categories is $>$ 0.99

Inaccurate Predictions :
```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  labels != validation$neo_class),"Validation : rpart",JPL_validation))
inaccurate_details %>% filter (model_name == "Validation : rpart") %>% .[,3:10] %>%
  tibble() %>% knitr::kable()
```

\newpage

## As Holdout
### WSRF

+   Illustration : Using the fit from the train set and treating the validation data as a holdout data.

Validation set matrix:
```{r message=FALSE, warning=FALSE, include=TRUE}
#process as matrix for the model
validation <- data_process(JPL_validation,1)
JPL_F <- data_process(JPL_data,1)
```

Prediction and the confusion matrix : 
```{r message=FALSE, warning=FALSE, include=TRUE}
y_f_wsrf <- predict(fit_wsrf, validation$X)
y_f_wsrf_m <- as.matrix(y_f_wsrf$class)
y_f_wsrf_m <- ifelse(y_f_wsrf_m == 1, 'NEO',ifelse(y_f_wsrf_m == 2, 'PHO','SO'))
cm <- confusionMatrix(as.factor(y_f_wsrf_m),validation$Y)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"Holdout : WSRF"))
cm$table
```

Results : 
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "Holdout : WSRF")  %>% knitr::kable()
```

Inaccurate Predictions :
```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  y_f_wsrf_m != validation$Y ),'Holdout : WSRF',JPL_validation))
inaccurate_details %>% filter (model_name == "Holdout : WSRF") %>% .[,3:10] %>%
  tibble() %>% knitr::kable()
```
\newpage

### Ranger

+   Illustration : Using the fit from the train set and treating the validation data as a holdout data.

Prediction and the confusion matrix : 
```{r Finalranger1, message=FALSE, warning=FALSE, cache=TRUE, include=TRUE}
set.seed(131825431, sample.kind = "Rounding")
#predict using the training fitted model against the validation set.
pred_O_ranger <- predict(fit_ranger, validation$X)
cm <- confusionMatrix(pred_O_ranger$predictions, validation$Y)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"Holdout : Ranger"))
cm$table
```


Results : 
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name =="Holdout : Ranger")  %>% knitr::kable()
```

+   The Recall and the F1 for the Ranger model on the validation dataset, as a holdout is $>$ 0.99

Inaccurate Predictions :
```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  pred_O_ranger$predictions != validation$Y ),'Holdout : Ranger',JPL_validation))
inaccurate_details %>% filter (model_name == "Holdout : Ranger") %>% .[,3:10] %>%
  tibble() %>% knitr::kable()
```

\newpage

### Gradient Boost

+   Illustration : Using the fit from the train set and treating the validation data as a holdout data.

Validation set data table:
```{r message=FALSE, warning=FALSE, include=TRUE}
#process as matrix for the model
validation <- data_process(JPL_validation,2)
```

Prediction and the confusion matrix:
```{r message=FALSE, warning=FALSE, include=TRUE}
y_f_gbm <- predict(fit_gbm, validation, n.trees = best.iter, type='response')
labels <- colnames(y_f_gbm)[apply(y_f_gbm, 1, which.max)]
cm <- confusionMatrix(factor(labels), validation$neo_class)
results <- bind_rows(results,results_process(as.data.frame(cm$byClass),"Holdout : gbm"))
cm$table
```

Results : 
```{r message=FALSE, warning=FALSE, include=TRUE}
results %>% filter (model_name == "Holdout : gbm")  %>% knitr::kable()
```

Inaccurate Predictions :
```{r message=FALSE, warning=FALSE, include=TRUE}
inaccurate_details <- rbind(inaccurate_details, inaccuracy_check(which(
  labels != validation$neo_class),'Holdout : gbm',JPL_validation))
inaccurate_details %>% filter (model_name == "Holdout : gbm") %>% .[,3:10] %>%
  tibble() %>% knitr::kable()
```

```{r message=FALSE, warning=FALSE, include=FALSE}
rm(.Random.seed,inaccuracy_check,inaccurate_details,JPL_F,labels,pred_ranger,validation,cm,descriptive_data,Final_rpart,gettuned,JPL_data,JPL_validation,pred_O_ranger,pred_rpart,results_process,rpart_cp,y_hat_rf,best.iter,data_process,git_gbm,fit_ranger,fit_wsrf,rf_mtry,y_f_gbm,y_f_wsrf,y_f_wsrf_m,fit_gbm)
gc()
```

\newpage

# Conclusion

The JPL small body dataset was downloaded from from the **https://ssd.jpl.nasa.gov/sbdb_query.cgi** website, cleaned and initial variable assessment is done using various techniques. 


Additional derived variables were also included for model evaluations. t-tests to check the statistical significance of the variations in each variable was also done separately ( _summary included in R code file_ ).
However, the details have not been included here in order to maintain concision of the case study document. 


In the model evaluation the multinom (PCA), Naive Bayes,  rpart, randomForest, wsrf, ranger and gbm models were eveluated. An ensemble of lda, qda, knn, rpart, svmRadialCost and wsrf was used for direction for a suitable model.


The classification of Small Bodies model was built using only 9 variables $H,q,moid,m\_d,e,t\_jup,a,i,T\_p$, of which 2 variables $m\_d \ and \ T\_p$ are derived variables.


The Final validation was done with The Ranger model (a fast implementation of random forests), WSRF (Forest of Weighted Subspace Decision Trees) and rpart (Recursive Partitioning and Regression Trees).


\hfill

**The Final Scores:**

**Recall**
```{r echo=FALSE, message=FALSE, warning=FALSE}
vh_models <- c("Validation : Ranger","Validation : WSRF","Validation : rpart",
                  "Holdout : Ranger","Holdout : WSRF","Holdout : gbm")
F1_score <- results %>% filter(model_name %in% vh_models) %>%
  select(model_name, F1, Object) %>% spread(.,Object,F1)
Recall_score <- results %>% filter(model_name %in% vh_models) %>%
  select(model_name, Recall, Object) %>% spread(.,Object,Recall)
Recall_score  %>% knitr::kable()
```

**F1**
```{r echo=FALSE, message=FALSE, warning=FALSE}
F1_score  %>% knitr::kable()
```


The Objective of the case study, to build a machine learning model to predict the classification of  the small solar bodies into $PHO$, $NEO$ and $SO$ (other small solar bodies), with the target measures of Recall for each category above 0.99 and F1 Score for each category above 0.99 was achieved.


```{r echo=FALSE, message=FALSE, warning=FALSE, fi}
par(mfrow=c(1,1))
results %>%  filter(model_name %in% vh_models) %>%
  ggplot(aes(F1,Recall, col=Object)) + geom_point(pch=19, cex=3)  +
  facet_wrap(~model_name, nrow=2, scales = "free") +
  theme(axis.text.x = element_text(angle=90,size=8, lineheight = 1.2, hjust=1),
        axis.text.y = element_text(size=8, lineheight = 1.2),legend.position = "bottom")
```



Further tweaking and tuning of the models and reassessing variables, both observed and derived might achieve an absolute Recall and F1 score of 1.

\hfill

## References

+   **Rafael A. Irizarry** Introduction to Data Science: **https://rafalab.github.io/dsbook/**

+   NASA JPL website **https://ssd.jpl.nasa.gov/**

+   Articles on https://wikipedia.org for the brief history details and the two images.


\hfill

\hfill $veneet.bhardwaj@gmail.com$

\newpage

## Annexure
Recall and F1 Scores of each model:

```{r echo=TRUE, message=FALSE, warning=FALSE}
results %>% knitr::kable()
```
```{r message=FALSE, warning=FALSE, include=FALSE}
rm(list = ls(all.names=TRUE))
gc()
```
\hfill veneet.bhardwaj@gmail.com